{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-17T02:54:37.865978Z","iopub.status.busy":"2024-06-17T02:54:37.865611Z","iopub.status.idle":"2024-06-17T02:54:57.492459Z","shell.execute_reply":"2024-06-17T02:54:57.491505Z","shell.execute_reply.started":"2024-06-17T02:54:37.865944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\n","Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n","Collecting trl\n","  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, tyro, bitsandbytes, trl, peft\n","Successfully installed bitsandbytes-0.43.1 peft-0.11.1 shtab-1.7.1 trl-0.9.4 tyro-0.8.4\n"]}],"source":["! pip install huggingface_hub transformers datasets peft trl bitsandbytes accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:54:57.494599Z","iopub.status.busy":"2024-06-17T02:54:57.494305Z","iopub.status.idle":"2024-06-17T02:55:17.013008Z","shell.execute_reply":"2024-06-17T02:55:17.012108Z","shell.execute_reply.started":"2024-06-17T02:54:57.494573Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-17 02:55:06.434466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-17 02:55:06.434575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-17 02:55:06.565435: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import re\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","from pathlib import Path\n","from random import randint\n","from huggingface_hub import login\n","from datasets import load_dataset, concatenate_datasets\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, pipeline\n","from trl import SFTTrainer, setup_chat_format\n","from peft import LoraConfig, AutoPeftModelForCausalLM"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:17.015295Z","iopub.status.busy":"2024-06-17T02:55:17.014558Z","iopub.status.idle":"2024-06-17T02:55:17.100852Z","shell.execute_reply":"2024-06-17T02:55:17.099951Z","shell.execute_reply.started":"2024-06-17T02:55:17.015260Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["login(\n","  token = \"your_token\",\n","  add_to_git_credential=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Iam using SFTTrainer which accepts input data in some particular formt. You can refer to the formats here - https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support. Iam using the conversational format which will be something like - \n","{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:17.104241Z","iopub.status.busy":"2024-06-17T02:55:17.103419Z","iopub.status.idle":"2024-06-17T02:55:17.111158Z","shell.execute_reply":"2024-06-17T02:55:17.110110Z","shell.execute_reply.started":"2024-06-17T02:55:17.104206Z"},"trusted":true},"outputs":[],"source":["def extract_schema_and_question(text):\n","    schema_pattern = r'\\[INST\\] Here is a database schema:(.*?)Please write me a SQL statement that answers the following question:'\n","    question_pattern = r'Please write me a SQL statement that answers the following question:(.*?)\\[/INST\\]'\n","\n","    schema_match = re.search(schema_pattern, text, re.DOTALL)\n","    question_match = re.search(question_pattern, text, re.DOTALL)\n","\n","    if schema_match and question_match:\n","        schema = schema_match.group(1).strip()\n","        question = question_match.group(1).strip()\n","        return schema, question\n","    else:\n","        return None, None"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:17.112851Z","iopub.status.busy":"2024-06-17T02:55:17.112331Z","iopub.status.idle":"2024-06-17T02:55:17.121451Z","shell.execute_reply":"2024-06-17T02:55:17.120571Z","shell.execute_reply.started":"2024-06-17T02:55:17.112820Z"},"trusted":true},"outputs":[],"source":["system_prompt = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","{schema}\"\"\"\n","\n","def create_instruction(sample):\n","    schema, question = extract_schema_and_question(sample[\"input\"])\n","    if schema and question:\n","        return {\n","        \"messages\": [\n","              {\"role\": \"system\", \"content\": system_prompt.format(schema = schema)},\n","              {\"role\": \"user\", \"content\": question},\n","              {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n","            ]\n","          }  \n","    else:\n","        return None"]},{"cell_type":"markdown","metadata":{},"source":["Iam using the dataset available at https://huggingface.co/datasets/lamini/bird_text_to_sql which contains the input with the schema of the database and query, and the output as the SQL query. Since I donot have access to any paid or larger GPUs, Iam training the model only on 1000 samples, which can be considered a decent size of samples to perform finetuning. "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:17.123125Z","iopub.status.busy":"2024-06-17T02:55:17.122824Z","iopub.status.idle":"2024-06-17T02:55:21.466574Z","shell.execute_reply":"2024-06-17T02:55:21.465664Z","shell.execute_reply.started":"2024-06-17T02:55:17.123102Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af6446c7a668413086fda4fe99c3454c","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/578 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04c5df192dff4e6b91ab9fa277ad20f2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.75M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3262ac4c41534aecb8faa6f01aa540a8","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/388k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c354d14da01f42cd8e795383c8372546","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/9428 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"322a42bb899546308308b7eb67a8c060","version_major":2,"version_minor":0},"text/plain":["Generating dev split:   0%|          | 0/1534 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'messages': [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nIngredient :\\ningredient_id [ INTEGER ] primary_key\\ncategory [ TEXT ]\\nname [ TEXT ]\\nplural [ TEXT ]\\n\\nRecipe :\\nrecipe_id [ INTEGER ] primary_key\\ntitle [ TEXT ]\\nsubtitle [ TEXT ]\\nservings [ INTEGER ]\\nyield_unit [ TEXT ]\\nprep_min [ INTEGER ]\\ncook_min [ INTEGER ]\\nstnd_min [ INTEGER ]\\nsource [ TEXT ]\\nintro [ TEXT ]\\ndirections [ TEXT ]\\n\\nNutrition :\\nrecipe_id [ INTEGER ] primary_key Nutrition.recipe_id = Recipe.recipe_id\\nprotein [ REAL ]\\ncarbo [ REAL ]\\nalcohol [ REAL ]\\ntotal_fat [ REAL ]\\nsat_fat [ REAL ]\\ncholestrl [ REAL ]\\nsodium [ REAL ]\\niron [ REAL ]\\nvitamin_c [ REAL ]\\nvitamin_a [ REAL ]\\nfiber [ REAL ]\\npcnt_cal_carb [ REAL ]\\npcnt_cal_fat [ REAL ]\\npcnt_cal_prot [ REAL ]\\ncalories [ REAL ]\\n\\nQuantity :\\nquantity_id [ INTEGER ] primary_key\\nrecipe_id [ INTEGER ] Quantity.recipe_id = Recipe.recipe_id\\ningredient_id [ INTEGER ] Quantity.ingredient_id = Ingredient.ingredient_id\\nmax_qty [ REAL ]\\nmin_qty [ REAL ]\\nunit [ TEXT ]\\npreparation [ TEXT ]\\noptional [ TEXT ]', 'role': 'system'}, {'content': 'What kind of preparation is needed for apple juice to make a raspberry-pear couscous cake?', 'role': 'user'}, {'content': \"SELECT T2.preparation FROM Recipe AS T1 INNER JOIN Quantity AS T2 ON T1.recipe_id = T2.recipe_id INNER JOIN Ingredient AS T3 ON T3.ingredient_id = T2.ingredient_id WHERE T1.title = 'Raspberry-Pear Couscous Cake' AND T3.name = 'apple juice';\", 'role': 'assistant'}]}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5057a63be267495da887c16e2f4584a6","version_major":2,"version_minor":0},"text/plain":["Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0bd1fa474304666921215b72f716ff4","version_major":2,"version_minor":0},"text/plain":["Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1347585"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset(\"lamini/bird_text_to_sql\")\n","\n","combined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"dev\"]])\n","\n","processed_dataset = combined_dataset.map(lambda x: create_instruction(x), remove_columns=combined_dataset.column_names)\n","\n","processed_dataset = processed_dataset.shuffle()\n","processed_dataset = processed_dataset.train_test_split(test_size = 0.15)\n","\n","print(processed_dataset[\"train\"][69])\n","\n","train_sampled = processed_dataset[\"train\"].select(range(1000))\n","test_sampled = processed_dataset[\"test\"].select(range(500))\n","\n","train_sampled.to_json(\"train_dataset.json\", orient=\"records\")\n","test_sampled.to_json(\"test_dataset.json\", orient=\"records\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:21.467987Z","iopub.status.busy":"2024-06-17T02:55:21.467704Z","iopub.status.idle":"2024-06-17T02:55:21.615931Z","shell.execute_reply":"2024-06-17T02:55:21.614997Z","shell.execute_reply.started":"2024-06-17T02:55:21.467963Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8baf0acb5534855a641cf09fcb6632c","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = load_dataset(\"json\", data_files = \"train_dataset.json\", split = \"train\")"]},{"cell_type":"markdown","metadata":{},"source":["Since I donot have access to any paid GPU services, Iam training the model on free GPU P100 instance provided by Kaggle, which is obviously not sufficient to train all 7 billion parameters of the model. So, I'm using LORA and quantization techniques in the training process. If you have access to larger GPUs, go ahead and experiment by increasing the lora_rank, set bf16 to True, increase the size of train data, and also you can include a technique called FLASH-ATTENTION which accelerates the training upto 3x. If you have access to google collabs paid version and A100 GPU, you can experiment with above parameters."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:55:21.617507Z","iopub.status.busy":"2024-06-17T02:55:21.617155Z","iopub.status.idle":"2024-06-17T02:58:53.777956Z","shell.execute_reply":"2024-06-17T02:58:53.777106Z","shell.execute_reply.started":"2024-06-17T02:55:21.617466Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"feb1d0ba5aef48c5b4ec14d557695ddf","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de6374b6051b4beda28e2982d54f47f1","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1a1f771e7134cd6ab0e3a8e8feae7dc","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a5f9d8a6f3b46c391c766d1fcdf881a","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8a10002123e4697a25f1efd617097dd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c125761d98c4af1b199752f749b25e1","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2415a05bec34254bd258ca516721abb","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cd711a84bdf41ca9633aebdae44631d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a834e157921946f9aeb93dfbb76cf40c","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe0d3203c5e846f1a6f65c978dd30dd9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/33.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63876357ad784aa5bf47c7b6e864ede9","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"587344303cc04ca692e5d39b6b035ca1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b1daa529a764605997e28708ecacb4a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_id = \"google/codegemma-7b\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=False,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16,\n","    quantization_config=bnb_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.padding_side = 'right'\n","model, tokenizer = setup_chat_format(model, tokenizer)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:58:53.779428Z","iopub.status.busy":"2024-06-17T02:58:53.779133Z","iopub.status.idle":"2024-06-17T02:58:53.783985Z","shell.execute_reply":"2024-06-17T02:58:53.782995Z","shell.execute_reply.started":"2024-06-17T02:58:53.779404Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    r=128,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:58:53.787486Z","iopub.status.busy":"2024-06-17T02:58:53.787196Z","iopub.status.idle":"2024-06-17T02:58:53.822929Z","shell.execute_reply":"2024-06-17T02:58:53.822259Z","shell.execute_reply.started":"2024-06-17T02:58:53.787462Z"},"trusted":true},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir=\"code-gemma-7b-text-to-sql\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=2,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=30,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=False,\n","    fp16=False,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.05,\n","    lr_scheduler_type=\"linear\",\n","    weight_decay=0.01\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:58:53.824139Z","iopub.status.busy":"2024-06-17T02:58:53.823860Z","iopub.status.idle":"2024-06-17T02:58:57.143879Z","shell.execute_reply":"2024-06-17T02:58:57.142940Z","shell.execute_reply.started":"2024-06-17T02:58:53.824116Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of  Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of  Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6142173b4e84ce7a8a31e7b9dde150c","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    max_seq_length=512,\n","    peft_config=peft_config,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,\n","        \"append_concat_token\": False,\n","    }\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:58:57.145485Z","iopub.status.busy":"2024-06-17T02:58:57.145135Z","iopub.status.idle":"2024-06-17T02:58:57.159976Z","shell.execute_reply":"2024-06-17T02:58:57.158980Z","shell.execute_reply.started":"2024-06-17T02:58:57.145451Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 51380224 || all params: 8589067264 || trainable%: 0.5982049321624686\n"]}],"source":["def print_trainable_parameters(model):\n","    trainable_params = 0\n","    all_param = model.num_parameters()\n","    for _, param in model.named_parameters():\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T02:58:57.161309Z","iopub.status.busy":"2024-06-17T02:58:57.160958Z","iopub.status.idle":"2024-06-17T04:47:21.548841Z","shell.execute_reply":"2024-06-17T04:47:21.547972Z","shell.execute_reply.started":"2024-06-17T02:58:57.161277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.1 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240617_025931-7htca3p8</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/llm-insights/huggingface/runs/7htca3p8' target=\"_blank\">code-gemma-7b-text-to-sql</a></strong> to <a href='https://wandb.ai/llm-insights/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/llm-insights/huggingface' target=\"_blank\">https://wandb.ai/llm-insights/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/llm-insights/huggingface/runs/7htca3p8' target=\"_blank\">https://wandb.ai/llm-insights/huggingface/runs/7htca3p8</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [402/402 1:47:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>30</td>\n","      <td>36.730400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.789800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.853200</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.587600</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.494900</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.442100</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.440400</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.426300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.350100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.353700</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.323500</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.325000</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.324800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=402, training_loss=3.250924762208663, metrics={'train_runtime': 6503.9821, 'train_samples_per_second': 0.247, 'train_steps_per_second': 0.062, 'total_flos': 3.851927038722048e+16, 'train_loss': 3.250924762208663, 'epoch': 1.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, training the model only ofr 1 epoch, that too with LORA and Quantization tokk me almost 2 hours. I tried increasing the Lora-rank, batch_size, and also max_seq_lenght parameters, but obviously it threw out of memory exception. As i mentioned earlier, If you have access to google collabs paid version and A100 GPU, you can increase the train data, and also train it for more epochs to get better results."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T04:47:31.369497Z","iopub.status.busy":"2024-06-17T04:47:31.368989Z","iopub.status.idle":"2024-06-17T04:50:51.877506Z","shell.execute_reply":"2024-06-17T04:50:51.876523Z","shell.execute_reply.started":"2024-06-17T04:47:31.369466Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"317c1f293a65404a9ccdb88fa05a4ab1","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/6.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Nishanth7803/code-gemma-7b-finetuned-text-to-sql/commit/5f2f5de3ebb294cf09cb1e978e054715654ffdc7', commit_message='Upload model', commit_description='', oid='5f2f5de3ebb294cf09cb1e978e054715654ffdc7', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["trainer.save_model()\n","trainer.model.push_to_hub(\"code-gemma-7b-finetuned-text-to-sql\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T04:50:51.879535Z","iopub.status.busy":"2024-06-17T04:50:51.879249Z","iopub.status.idle":"2024-06-17T04:50:51.896956Z","shell.execute_reply":"2024-06-17T04:50:51.895895Z","shell.execute_reply.started":"2024-06-17T04:50:51.879509Z"},"trusted":true},"outputs":[],"source":["del model\n","del trainer"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T04:54:39.123124Z","iopub.status.busy":"2024-06-17T04:54:39.122733Z","iopub.status.idle":"2024-06-17T04:54:39.128972Z","shell.execute_reply":"2024-06-17T04:54:39.127944Z","shell.execute_reply.started":"2024-06-17T04:54:39.123092Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T04:54:41.593261Z","iopub.status.busy":"2024-06-17T04:54:41.592416Z","iopub.status.idle":"2024-06-17T04:59:17.516722Z","shell.execute_reply":"2024-06-17T04:59:17.515820Z","shell.execute_reply.started":"2024-06-17T04:54:41.593226Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1365: UserWarning: Current model requires 7168 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bdb86eea26d43abadef94e04ae4e57c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["peft_model_id = \"/kaggle/working/code-gemma-7b-text-to-sql\"\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","  peft_model_id,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T05:00:04.791401Z","iopub.status.busy":"2024-06-17T05:00:04.790723Z","iopub.status.idle":"2024-06-17T05:03:59.104295Z","shell.execute_reply":"2024-06-17T05:03:59.103121Z","shell.execute_reply.started":"2024-06-17T05:00:04.791367Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01be23055e1b46239cb7c6cc99f145d2","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Schema:\n","You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","Examination :\n","ID [ INTEGER ] Examination.ID = Patient.ID\n","Examination Date [ DATE ]\n","aCL IgG [ REAL ]\n","aCL IgM [ REAL ]\n","ANA [ INTEGER ]\n","ANA Pattern [ TEXT ]\n","aCL IgA [ INTEGER ]\n","Diagnosis [ TEXT ]\n","KCT [ TEXT ]\n","RVVT [ TEXT ]\n","LAC [ TEXT ]\n","Symptoms [ TEXT ]\n","Thrombosis [ INTEGER ]\n","\n","Patient :\n","ID [ INTEGER ] primary_key\n","SEX [ TEXT ]\n","Birthday [ DATE ]\n","Description [ DATE ]\n","First Date [ DATE ]\n","Admission [ TEXT ]\n","Diagnosis [ TEXT ]\n","\n","Laboratory :\n","ID [ INTEGER ] Laboratory.ID = Patient.ID\n","Date [ DATE ]\n","GOT [ INTEGER ]\n","GPT [ INTEGER ]\n","LDH [ INTEGER ]\n","ALP [ INTEGER ]\n","TP [ REAL ]\n","ALB [ REAL ]\n","UA [ REAL ]\n","UN [ INTEGER ]\n","CRE [ REAL ]\n","T-BIL [ REAL ]\n","T-CHO [ INTEGER ]\n","TG [ INTEGER ]\n","CPK [ INTEGER ]\n","GLU [ INTEGER ]\n","WBC [ REAL ]\n","RBC [ REAL ]\n","HGB [ REAL ]\n","HCT [ REAL ]\n","PLT [ INTEGER ]\n","PT [ REAL ]\n","APTT [ INTEGER ]\n","FG [ REAL ]\n","PIC [ INTEGER ]\n","TAT [ INTEGER ]\n","TAT2 [ INTEGER ]\n","U-PRO [ TEXT ]\n","IGG [ INTEGER ]\n","IGA [ INTEGER ]\n","IGM [ INTEGER ]\n","CRP [ TEXT ]\n","RA [ TEXT ]\n","RF [ TEXT ]\n","C3 [ INTEGER ]\n","C4 [ INTEGER ]\n","RNP [ TEXT ]\n","SM [ TEXT ]\n","SC170 [ TEXT ]\n","SSA [ TEXT ]\n","SSB [ TEXT ]\n","CENTROMEA [ TEXT ]\n","DNA [ TEXT ]\n","DNA-II [ INTEGER ]\n","Query:\n","Excluding all P only ANA Pattern patients, how many of the remainder are women born between 1980 and 1989?\n","Original Answer:\n","SELECT COUNT(DISTINCT T1.ID) FROM Patient AS T1 INNER JOIN Examination AS T2 ON T1.ID = T2.ID WHERE T2.`ANA Pattern` != 'P' AND STRFTIME('%Y', T1.Birthday) BETWEEN '1980' AND '1989' AND T1.SEX = 'F';\n","Generated Answer:\n","SELECT COUNT(T1.ID) FROM Patient AS T1 INNER JOIN Examination AS T2 ON T1.ID = T2.ID WHERE T1.SEX = 'F' AND T1.Birthday BETWEEN '1980-01-01' AND '1989-12-31' AND T2.aCL IgM = 0 AND T2.aCL IgG = 0 AND T2.ANA = 0;\n"]}],"source":["eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n","rand_idx = randint(0, len(eval_dataset))\n","\n","# Test on sample \n","prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","\n","print(f\"Schema:\\n{eval_dataset[rand_idx]['messages'][0]['content']}\")\n","print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n","print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n","print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"]},{"cell_type":"markdown","metadata":{},"source":["You can see that after training for only 1 epoch, and only on 1000 samples, our model outputs not very perfect result, but a decent resut I would say. Obviously more improvements can be done, so go ahead and try experimenting with different parameters. If you think any other improvements can be made, feel free to drop a mail to nishanth.annamdevula7803@gmail.com\n","\n","HAPPY BUILDING :)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
